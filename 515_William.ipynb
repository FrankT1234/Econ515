{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBCSDOP9yoCm9yBVzalyGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuhengWillZhao/Econ515_William/blob/main/515_William.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Ja72ozajUb_y",
        "outputId": "c187251c-2646-4d4c-ea86-bd52c28fae37"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'spot.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-665168041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# ----------------- Load data (paths per your upload) -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mspot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_price_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spot.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_price_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'future.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtbill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_price_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TB3MS (1).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-665168041.py\u001b[0m in \u001b[0;36mload_price_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# ----------------- Utility: robust CSV loader -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_price_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# normalize column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spot.csv'"
          ]
        }
      ],
      "source": [
        "# Revised full code: display-only (no file writes)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from scipy.stats import chi2\n",
        "from datetime import datetime\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "# ----------------- Utility: robust CSV loader -----------------\n",
        "def load_price_csv(path):\n",
        "    tmp = pd.read_csv(path)\n",
        "    # normalize column names\n",
        "    cols = [c.strip() for c in tmp.columns]\n",
        "    tmp.columns = cols\n",
        "    lowcols = [c.lower() for c in cols]\n",
        "    # date column\n",
        "    if 'date' in lowcols:\n",
        "        date_col = cols[lowcols.index('date')]\n",
        "    elif 'data' in lowcols:\n",
        "        date_col = cols[lowcols.index('data')]\n",
        "    else:\n",
        "        date_col = cols[0]\n",
        "    # price column\n",
        "    if 'price' in lowcols:\n",
        "        price_col = cols[lowcols.index('price')]\n",
        "    else:\n",
        "        price_col = [c for c in cols if c != date_col][0]\n",
        "    df = tmp[[date_col, price_col]].rename(columns={date_col:'date', price_col:'price'})\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df = df.set_index('date').sort_index()\n",
        "    return df\n",
        "\n",
        "# ----------------- Load data (paths per your upload) -----------------\n",
        "spot = load_price_csv('spot.csv')\n",
        "future = load_price_csv('future.csv')\n",
        "tbill = load_price_csv('TB3MS (1).csv')\n",
        "\n",
        "# ----------------- Align to sample and business days -----------------\n",
        "start_date = pd.to_datetime('2020-01-01')\n",
        "end_date = pd.to_datetime('2024-03-31')\n",
        "idx = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "\n",
        "\n",
        "spot = spot.reindex(idx).rename(columns={'price':'spot'})\n",
        "future = future.reindex(idx).rename(columns={'price':'future'})\n",
        "tbill = tbill.reindex(idx).rename(columns={'price':'tbill'})\n",
        "\n",
        "# forward-fill then backfill\n",
        "spot['spot'] = spot['spot'].ffill().bfill()\n",
        "future['future'] = future['future'].ffill().bfill()\n",
        "tbill['tbill'] = tbill['tbill'].ffill().bfill()\n",
        "\n",
        "# combine\n",
        "df = pd.concat([spot['spot'], future['future'], tbill['tbill']], axis=1).dropna()\n",
        "\n",
        "# logs, diffs, and Rt\n",
        "df['log_spot'] = np.log(df['spot'])\n",
        "df['log_future'] = np.log(df['future'])\n",
        "df['dlog_spot'] = df['log_spot'].diff()\n",
        "df['dlog_future'] = df['log_future'].diff()\n",
        "df['Rt'] = df['tbill'] / 1200.0\n",
        "df = df.dropna()\n",
        "\n",
        "# ----------------- Display: head -----------------\n",
        "print(\"Prepared data (head):\")\n",
        "display(df[['spot','future','tbill','Rt']].head())\n",
        "\n",
        "# ----------------- Unit root tests (ADF, KPSS) -----------------\n",
        "def do_ur_tests(series):\n",
        "    out = {}\n",
        "    s = series.dropna()\n",
        "    try:\n",
        "        adf = adfuller(s, autolag='AIC')\n",
        "        out['ADF_stat'] = adf[0]; out['ADF_pvalue'] = adf[1]\n",
        "    except Exception:\n",
        "        out['ADF_stat'] = np.nan; out['ADF_pvalue'] = np.nan\n",
        "    try:\n",
        "        kps = kpss(s, regression='c', nlags='auto')\n",
        "        out['KPSS_stat'] = kps[0]; out['KPSS_pvalue'] = kps[1]\n",
        "    except Exception:\n",
        "        out['KPSS_stat'] = np.nan; out['KPSS_pvalue'] = np.nan\n",
        "    return out\n",
        "\n",
        "ur_results = {\n",
        "    'log_spot': do_ur_tests(df['log_spot']),\n",
        "    'log_future': do_ur_tests(df['log_future']),\n",
        "    'dlog_spot': do_ur_tests(df['dlog_spot']),\n",
        "    'dlog_future': do_ur_tests(df['dlog_future'])\n",
        "}\n",
        "ur_table = pd.DataFrame(ur_results).T\n",
        "print(\"\\nUnit-root test summary (ADF, KPSS):\")\n",
        "display(ur_table)\n",
        "\n",
        "# ----------------- Johansen cointegration (trace) -----------------\n",
        "print(\"\\nJohansen cointegration (trace) baseline:\")\n",
        "jres = coint_johansen(df[['log_spot','log_future']], det_order=0, k_ar_diff=1)\n",
        "jtab = []\n",
        "for i,(lr,val) in enumerate(zip(jres.lr1, jres.eig)):\n",
        "    jtab.append({'r<=%d' % i: i, 'trace_stat': lr, 'eigenvalue': val,\n",
        "                 'crit_90': jres.cvt[i,0], 'crit_95': jres.cvt[i,1], 'crit_99': jres.cvt[i,2]})\n",
        "display(pd.DataFrame(jtab))\n",
        "\n",
        "# ----------------- Full-sample OLS cointegrating regression -----------------\n",
        "X_full = sm.add_constant(df['log_future'])\n",
        "ols_full = sm.OLS(df['log_spot'], X_full).fit()\n",
        "ols_full_hac = ols_full.get_robustcov_results(cov_type='HAC', maxlags=12)\n",
        "full_table = pd.DataFrame({\n",
        "    'estimate': ols_full.params,\n",
        "    'hac_se': ols_full_hac.bse,\n",
        "    't': ols_full_hac.tvalues,\n",
        "    'pvalue': ols_full_hac.pvalues\n",
        "})\n",
        "print(\"\\nFull-sample OLS cointegrating regression (HAC SEs):\")\n",
        "display(full_table.T)\n",
        "\n",
        "# residuals\n",
        "df['coint_resid'] = ols_full.resid.values\n",
        "\n",
        "# ----------------- Bai–Perron style segmentation (dynamic programming) -----------------\n",
        "T = len(df)\n",
        "trim = int(np.floor(0.15 * T))   # 15% trimming\n",
        "max_breaks = 5\n",
        "y = df['log_spot'].values\n",
        "X = sm.add_constant(df['log_future']).values\n",
        "\n",
        "# SSR precompute\n",
        "min_len = 4\n",
        "SSR = np.full((T+1, T+1), np.inf)\n",
        "for i in range(0, T):\n",
        "    for j in range(i+min_len, T+1):\n",
        "        Xi = X[i:j,:]; yi = y[i:j]\n",
        "        try:\n",
        "            b = np.linalg.lstsq(Xi, yi, rcond=None)[0]\n",
        "            resid = yi - Xi.dot(b)\n",
        "            SSR[i,j] = float(np.sum(resid**2))\n",
        "        except Exception:\n",
        "            SSR[i,j] = np.inf\n",
        "\n",
        "# dynamic programming for partitions\n",
        "def best_partition(m):\n",
        "    K = m+1\n",
        "    dp = np.full((K+1, T+1), np.inf)\n",
        "    prev = -np.ones((K+1, T+1), dtype=int)\n",
        "    dp[0,0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        jmin = k*trim\n",
        "        jmax = T - (K-k)*trim\n",
        "        for j in range(jmin, jmax+1):\n",
        "            best = np.inf; arg = -1\n",
        "            i_min = (k-1)*trim\n",
        "            i_max = j - trim\n",
        "            for i in range(i_min, i_max+1):\n",
        "                s = SSR[i,j]\n",
        "                if np.isinf(s): continue\n",
        "                cand = dp[k-1,i] + s\n",
        "                if cand < best:\n",
        "                    best = cand; arg = i\n",
        "            dp[k,j] = best; prev[k,j] = arg\n",
        "    if np.isinf(dp[K,T]):\n",
        "        return None, np.inf\n",
        "    # reconstruct\n",
        "    ends = []; j=T; k=K\n",
        "    while k>0:\n",
        "        i = prev[k,j]\n",
        "        ends.append(j)\n",
        "        j = i; k -= 1\n",
        "    ends.reverse()\n",
        "    break_positions = ends[:-1]\n",
        "    return break_positions, dp[K,T]\n",
        "\n",
        "results = {}\n",
        "for m in range(0, max_breaks+1):\n",
        "    bpos, ssr = best_partition(m)\n",
        "    results[m] = {'break_positions': bpos, 'total_ssr': ssr}\n",
        "\n",
        "# show SSR vs m and break dates\n",
        "ssr_list = []\n",
        "breaks_list = []\n",
        "for m in range(0, max_breaks+1):\n",
        "    bp = results[m]['break_positions']\n",
        "    ssr_list.append(results[m]['total_ssr'])\n",
        "    if bp:\n",
        "        breaks_list.append([df.index[p-1].strftime('%Y-%m-%d') for p in bp])\n",
        "    else:\n",
        "        breaks_list.append([])\n",
        "\n",
        "print(\"\\nSegmentation results (SSR by m) and selected partition candidate breaks:\")\n",
        "seg_df = pd.DataFrame({'m': list(range(0, max_breaks+1)), 'SSR': ssr_list, 'breaks': breaks_list})\n",
        "display(seg_df)\n",
        "\n",
        "# plot SSR vs m\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(seg_df['m'], seg_df['SSR'], marker='o')\n",
        "plt.xlabel('Number of breaks (m)')\n",
        "plt.ylabel('Total SSR (minimized)')\n",
        "plt.title('SSR minimized vs number of breaks (Bai–Perron DP)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ----------------- Compute raw Wald for chosen partitions (approx sup-Wald) -----------------\n",
        "def estimate_regimes_and_wald(break_positions):\n",
        "    starts = [0] + (break_positions if break_positions else [])\n",
        "    ends = (break_positions if break_positions else []) + [T]\n",
        "    params = []\n",
        "    covs = []\n",
        "    SSR_seg = 0.0\n",
        "    for s,e in zip(starts, ends):\n",
        "        Xi = X[s:e,:]; yi = y[s:e]\n",
        "        beta = np.linalg.lstsq(Xi, yi, rcond=None)[0]\n",
        "        params.append(beta)\n",
        "        resid = yi - Xi.dot(beta)\n",
        "        SSR_seg += float(np.sum(resid**2))\n",
        "        # HAC cov via statsmodels on the sub-sample\n",
        "        sub = df.iloc[s:e]\n",
        "        ols_sub = sm.OLS(sub['log_spot'], sm.add_constant(sub['log_future'])).fit()\n",
        "        robust = ols_sub.get_robustcov_results(cov_type='HAC', maxlags=12)\n",
        "        covs.append(np.asarray(robust.cov_params()))\n",
        "    pooled_beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "    # stacked differences\n",
        "    stacked = np.concatenate([p - pooled_beta for p in params], axis=0)\n",
        "    # block-diagonal covariance\n",
        "    k = params[0].shape[0]; m = len(params)\n",
        "    bigcov = np.zeros((m*k, m*k))\n",
        "    for i in range(m):\n",
        "        bigcov[i*k:(i+1)*k, i*k:(i+1)*k] = covs[i]\n",
        "    try:\n",
        "        inv_bigcov = np.linalg.pinv(bigcov)\n",
        "        wald = float(stacked.T.dot(inv_bigcov.dot(stacked)))\n",
        "    except Exception:\n",
        "        wald = np.nan\n",
        "    return {'wald': wald, 'SSRp': float(np.sum((y - X.dot(pooled_beta))**2)), 'SSRseg': SSR_seg, 'params': params, 'covs': covs}\n",
        "\n",
        "sup_wald = {}\n",
        "for m in range(1, max_breaks+1):\n",
        "    bp = results[m]['break_positions']\n",
        "    if bp is None:\n",
        "        sup_wald[m] = {'wald': np.nan, 'break_positions': bp}\n",
        "    else:\n",
        "        est = estimate_regimes_and_wald(bp)\n",
        "        sup_wald[m] = {'wald': est['wald'], 'break_positions': bp, 'SSRseg': est['SSRseg'], 'SSRp': est['SSRp']}\n",
        "\n",
        "print(\"\\nRaw Wald stats for candidate partitions (per m):\")\n",
        "rows = []\n",
        "for m in range(1, max_breaks+1):\n",
        "    bp = sup_wald[m]['break_positions']\n",
        "    dates = [df.index[p-1].strftime('%Y-%m-%d') for p in bp] if bp else []\n",
        "    rows.append({'m': m, 'raw_wald': sup_wald[m]['wald'], 'breaks': dates})\n",
        "display(pd.DataFrame(rows))\n",
        "\n",
        "# choose m_hat by UDmax (max raw_wald)\n",
        "valid = {m: sup_wald[m]['wald'] for m in sup_wald if not np.isnan(sup_wald[m]['wald'])}\n",
        "if len(valid)==0:\n",
        "    m_hat = 0\n",
        "else:\n",
        "    m_hat = max(valid, key=valid.get)\n",
        "print(\"\\nSelected number of breaks m_hat (UDmax raw-wald):\", m_hat)\n",
        "chosen_bp = sup_wald[m_hat]['break_positions'] if m_hat>0 else []\n",
        "chosen_dates = [df.index[p-1].strftime('%Y-%m-%d') for p in chosen_bp] if chosen_bp else []\n",
        "print(\"Chosen break dates:\", chosen_dates)\n",
        "\n",
        "# ----------------- Bootstrap empirical p-value for sup-Wald (residual bootstrap) -----------------\n",
        "def bootstrap_sup_wald(break_positions, nboot=400, seed=1234):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    pooled_beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
        "    resid_pool = y - X.dot(pooled_beta)\n",
        "    obs_wald = sup_wald[m_hat]['wald']\n",
        "    w_star = []\n",
        "    for b in range(nboot):\n",
        "        e_star = rng.choice(resid_pool, size=T, replace=True)\n",
        "        y_star = X.dot(pooled_beta) + e_star\n",
        "        # compute regime betas & covs on y_star for the chosen partition\n",
        "        # small speed-up: compute OLS per regime and homoskedastic cov approx\n",
        "        starts = [0] + (break_positions if break_positions else [])\n",
        "        ends = (break_positions if break_positions else []) + [T]\n",
        "        params_star = []\n",
        "        bigcov_star = np.zeros((len(starts)*2, len(starts)*2))\n",
        "        stacked_diff = []\n",
        "        for i,(s,e) in enumerate(zip(starts, ends)):\n",
        "            Xi = X[s:e,:]; yi_star = y_star[s:e]\n",
        "            beta_star = np.linalg.lstsq(Xi, yi_star, rcond=None)[0]\n",
        "            params_star.append(beta_star)\n",
        "            resid_seg = yi_star - Xi.dot(beta_star)\n",
        "            sigma2 = np.var(resid_seg, ddof=Xi.shape[1])\n",
        "            cov_beta = sigma2 * np.linalg.pinv(Xi.T.dot(Xi))\n",
        "            bigcov_star[i*2:(i+1)*2, i*2:(i+1)*2] = cov_beta\n",
        "            stacked_diff.append(beta_star - pooled_beta)\n",
        "        stacked = np.concatenate(stacked_diff, axis=0)\n",
        "        try:\n",
        "            inv_bigcov = np.linalg.pinv(bigcov_star)\n",
        "            w = float(stacked.T.dot(inv_bigcov.dot(stacked)))\n",
        "        except Exception:\n",
        "            w = np.nan\n",
        "        w_star.append(w)\n",
        "    w_star = np.array(w_star)\n",
        "    pval = np.mean(w_star >= obs_wald)\n",
        "    return {'obs_wald': obs_wald, 'wald_star': w_star, 'pvalue': pval}\n",
        "\n",
        "if m_hat > 0:\n",
        "    print(\"\\nRunning residual bootstrap (this will take ~1-2 minutes depending on nboot)...\")\n",
        "    boot = bootstrap_sup_wald(chosen_bp, nboot=400, seed=2025)\n",
        "    print(\"Bootstrap empirical p-value for sup-Wald (chosen partition):\", boot['pvalue'])\n",
        "    # histogram\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.histplot(boot['wald_star'], bins=40, kde=False)\n",
        "    plt.axvline(boot['obs_wald'], color='red', linestyle='--', label=f'Observed Wald={boot[\"obs_wald\"]:.2f}')\n",
        "    plt.xlabel('Bootstrap Wald*')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Bootstrap distribution of Wald* (residual bootstrap)\\nObserved Wald marked in red')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nNo breaks selected (m_hat=0) — skipping bootstrap.\")\n",
        "\n",
        "# ----------------- Display cointegration residuals with chosen breaks -----------------\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(df.index, df['coint_resid'], label='cointegration residuals')\n",
        "for b in chosen_bp:\n",
        "    plt.axvline(df.index[b-1], color='red', linestyle='--', linewidth=1)\n",
        "plt.title('Cointegration residuals with chosen breakpoints (vertical dashed lines)')\n",
        "plt.xlabel('Date')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ----------------- Regime-wise OLS estimates (final) -----------------\n",
        "regimes = []\n",
        "start_idx = 0\n",
        "parts = (chosen_bp if chosen_bp else []) + [T]\n",
        "for end in parts:\n",
        "    sub = df.iloc[start_idx:end]\n",
        "    Xsub = sm.add_constant(sub['log_future'])\n",
        "    ols_sub = sm.OLS(sub['log_spot'], Xsub).fit()\n",
        "    ols_sub_hac = ols_sub.get_robustcov_results(cov_type='HAC', maxlags=12)\n",
        "    regimes.append({\n",
        "        'start': sub.index[0].strftime('%Y-%m-%d'),\n",
        "        'end': sub.index[-1].strftime('%Y-%m-%d'),\n",
        "        'coef_const': float(ols_sub.params['const']),\n",
        "        'coef_beta': float(ols_sub.params['log_future']),\n",
        "        'se_const': float(ols_sub_hac.bse[0]),\n",
        "        'se_beta': float(ols_sub_hac.bse[1]),\n",
        "        'nobs': len(sub)\n",
        "    })\n",
        "    start_idx = end\n",
        "\n",
        "reg_df = pd.DataFrame(regimes)\n",
        "print(\"\\nRegime-wise cointegration estimates (HAC SEs):\")\n",
        "display(reg_df)\n",
        "\n",
        "# plot coefficient bar chart with error bars\n",
        "plt.figure(figsize=(8,4))\n",
        "x_positions = np.arange(len(reg_df))\n",
        "plt.errorbar(x_positions, reg_df['coef_beta'], yerr=1.96*reg_df['se_beta'], fmt='o', capsize=5)\n",
        "plt.xticks(x_positions, [f\"{r['start']}→{r['end']}\" for _,r in reg_df.iterrows()], rotation=30, ha='right')\n",
        "plt.ylabel('Beta (cointegrating slope)')\n",
        "plt.title('Regime-wise cointegration slope estimates (95% CI)')\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ----------------- AK-like LM approximation and diagnostic -----------------\n",
        "resid = df['coint_resid'].values\n",
        "best_p = 1; best_aic = 1e12; best_model = None\n",
        "for p in range(1,7):\n",
        "    try:\n",
        "        m_ar = AutoReg(resid, lags=p, old_names=False).fit()\n",
        "        if hasattr(m_ar,'aic') and m_ar.aic < best_aic:\n",
        "            best_aic = m_ar.aic; best_p = p; best_model = m_ar\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if best_model is not None:\n",
        "    if hasattr(best_model, 'rsquared'):\n",
        "        R2 = best_model.rsquared\n",
        "    else:\n",
        "        SSR_ar = np.sum(best_model.resid**2)\n",
        "        var_resid = np.sum((resid - resid.mean())**2)\n",
        "        R2 = 1 - SSR_ar / var_resid\n",
        "    LM_stat = len(resid) * R2\n",
        "    pval_LM = 1 - chi2.cdf(LM_stat, df=best_p)\n",
        "else:\n",
        "    LM_stat = np.nan; pval_LM = np.nan\n",
        "\n",
        "print(\"\\nAK-like LM approximation (informative):\")\n",
        "print(f\" Selected AR lag p = {best_p}\")\n",
        "print(f\" LM_stat ≈ {LM_stat:.4f}, approx p-value (chi2 df={best_p}) ≈ {pval_LM:.6g}\")\n",
        "\n",
        "# quick ACF plot of residuals for diagnostics\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "plt.figure(figsize=(8,3))\n",
        "plot_acf(resid, lags=30, alpha=0.05)\n",
        "plt.title('ACF of cointegration residuals (diagnostic for AK-LM)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ----------------- Final summary -----------------\n",
        "# ----------------- Final summary -----------------\n",
        "print(\"\\n--- Summary (presentation-ready) ---\")\n",
        "\n",
        "# Retrieve HAC SEs correctly by position\n",
        "beta_se = float(ols_full_hac.bse[1])   # 2nd element corresponds to 'log_future'\n",
        "const_se = float(ols_full_hac.bse[0])\n",
        "\n",
        "print(f\" Sample: {start_date.date()} to {end_date.date()}, {T} business-day obs used (after trimming NAs).\")\n",
        "print(f\" Full-sample cointegrating slope (beta): {ols_full.params['log_future']:.6f} (HAC se {beta_se:.6f})\")\n",
        "print(f\" Constant term: {ols_full.params['const']:.6f} (HAC se {const_se:.6f})\")\n",
        "print(f\" Selected number of breaks (m_hat): {m_hat}; break dates: {chosen_dates}\")\n",
        "if m_hat > 0:\n",
        "    print(f\" Bootstrap empirical p-value for sup-Wald (chosen partition): {pval_boot:.4f}\")\n",
        "print(f\" AK-like LM approx stat: {LM_stat:.4f}, approx p-value: {pval_LM:.6g}\")\n"
      ]
    }
  ]
}